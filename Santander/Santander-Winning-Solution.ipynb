{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Santander-Winning-Solution# Santander-Winning-Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T22:04:15.821300Z",
     "start_time": "2019-04-26T22:04:14.765612Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89003#latest-520734\n",
    "import fastai\n",
    "import time\n",
    "from fastai.tabular import *\n",
    "from fastai.text import *\n",
    "#import feather\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "from fastai.callbacks import SaveModelCallback\n",
    "import logging\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T22:04:15.825478Z",
     "start_time": "2019-04-26T22:04:15.822807Z"
    }
   },
   "outputs": [],
   "source": [
    "#logger\n",
    "def get_logger():\n",
    "    FORMAT = '[%(levelname)s]%(asctime)s:%(name)s:%(message)s'\n",
    "    logging.basicConfig(format=FORMAT)\n",
    "    logger = logging.getLogger('main')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T22:04:15.838222Z",
     "start_time": "2019-04-26T22:04:15.826614Z"
    }
   },
   "outputs": [],
   "source": [
    "def auroc_score(input, target):\n",
    "    input, target = input.cpu().numpy()[:,1], target.cpu().numpy()\n",
    "    return roc_auc_score(target, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T22:04:16.081149Z",
     "start_time": "2019-04-26T22:04:16.068211Z"
    }
   },
   "outputs": [],
   "source": [
    "# Callback to calculate AUC at the end of each epoch\n",
    "class AUROC(Callback):\n",
    "    _order = -20 #Needs to run before the recorder\n",
    "\n",
    "    def __init__(self, learn, **kwargs): self.learn = learn\n",
    "    def on_train_begin(self, **kwargs): self.learn.recorder.add_metric_names(['AUROC'])\n",
    "    def on_epoch_begin(self, **kwargs): self.output, self.target = [], []\n",
    "    \n",
    "    def on_batch_end(self, last_target, last_output, train, **kwargs):\n",
    "        if not train:\n",
    "            self.output.append(last_output)\n",
    "            self.target.append(last_target)\n",
    "                \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        if len(self.output) > 0:\n",
    "            output = torch.cat(self.output)\n",
    "            target = torch.cat(self.target)\n",
    "            preds = F.softmax(output, dim=1)\n",
    "            metric = auroc_score(preds, target)\n",
    "            return add_metrics(last_metrics, [metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T22:04:16.725845Z",
     "start_time": "2019-04-26T22:04:16.710561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Callback that do the shuffle augmentation        \n",
    "class AugShuffCallback(LearnerCallback):\n",
    "    def __init__(self, learn:Learner):\n",
    "        super().__init__(learn)\n",
    "        \n",
    "    def on_batch_begin(self, last_input, last_target, train, **kwargs):\n",
    "        if not train: return\n",
    "        m_pos = last_target==1\n",
    "        m_neg = last_target==0\n",
    "        \n",
    "        pos_cat = last_input[0][m_pos]\n",
    "        pos_cont = last_input[1][m_pos]\n",
    "        \n",
    "        neg_cat = last_input[0][m_neg]\n",
    "        neg_cont = last_input[1][m_neg]\n",
    "        \n",
    "        for f in range(200):\n",
    "            shuffle_pos = torch.randperm(pos_cat.size(0)).to(last_input[0].device)\n",
    "            pos_cat[:,f] = pos_cat[shuffle_pos,f]\n",
    "            pos_cont[:,f] = pos_cont[shuffle_pos, f]\n",
    "            pos_cont[:,f+200] = pos_cont[shuffle_pos, f+200]\n",
    "            \n",
    "            shuffle_neg = torch.randperm(neg_cat.size(0)).to(last_input[0].device)\n",
    "            neg_cat[:,f] = neg_cat[shuffle_neg,f]\n",
    "            neg_cont[:, f] = neg_cont[shuffle_neg, f]\n",
    "            neg_cont[:,f+200] = neg_cont[shuffle_neg, f+200]\n",
    "        \n",
    "        new_input = [torch.cat([pos_cat, neg_cat]), torch.cat([pos_cont, neg_cont])]\n",
    "        new_target = torch.cat([last_target[m_pos], last_target[m_neg]])\n",
    "        \n",
    "        return {'last_input': new_input, 'last_target': new_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T22:04:17.949793Z",
     "start_time": "2019-04-26T22:04:17.934156Z"
    }
   },
   "outputs": [],
   "source": [
    "# Just a longer version of the random sampler : each samples is given \"mult\" times.\n",
    "class LongerRandomSampler(Sampler):\n",
    "    def __init__(self, data_source, replacement=False, num_samples=None, mult=3):\n",
    "        self.data_source = data_source\n",
    "        self.replacement = replacement\n",
    "        self.num_samples = num_samples\n",
    "        self.mult = mult\n",
    "\n",
    "        if self.num_samples is not None and replacement is False:\n",
    "            raise ValueError(\"With replacement=False, num_samples should not be specified, \"\n",
    "                             \"since a random permute will be performed.\")\n",
    "\n",
    "        if self.num_samples is None:\n",
    "            self.num_samples = len(self.data_source) * self.mult\n",
    "\n",
    "        if not isinstance(self.num_samples, int) or self.num_samples <= 0:\n",
    "            raise ValueError(\"num_samples should be a positive integeral \"\n",
    "                             \"value, but got num_samples={}\".format(self.num_samples))\n",
    "        if not isinstance(self.replacement, bool):\n",
    "            raise ValueError(\"replacement should be a boolean value, but got \"\n",
    "                             \"replacement={}\".format(self.replacement))\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = len(self.data_source)\n",
    "        if self.replacement:\n",
    "            return iter(torch.randint(high=n, size=(self.num_samples*self.mult,), dtype=torch.int64).tolist())\n",
    "        return iter(torch.randperm(n).tolist()*self.mult)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)*self.mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T22:04:19.059283Z",
     "start_time": "2019-04-26T22:04:19.035262Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the NN structure, starting from fast.ai TabularModel.\n",
    "class my_TabularModel(nn.Module):\n",
    "    \"Basic model for tabular data.\"\n",
    "    def __init__(self, emb_szs:ListSizes, n_cont:int, out_sz:int, layers:Collection[int], ps:Collection[float]=None,\n",
    "                 emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, bn_final:bool=False, \n",
    "                 cont_emb=2, cont_emb_notu=2):\n",
    "        \n",
    "        super().__init__()\n",
    "        # \"Continuous embedding NN for raw features\"\n",
    "        self.cont_emb = cont_emb[1]\n",
    "        self.cont_emb_l = torch.nn.Linear(1 + 2, cont_emb[0])\n",
    "        self.cont_emb_l2 = torch.nn.Linear(cont_emb[0], cont_emb[1])\n",
    "        \n",
    "        # \"Continuous embedding NN for \"not unique\" features\". cf #1 solution post\n",
    "        self.cont_emb_notu_l = torch.nn.Linear(1 + 2, cont_emb_notu[0])\n",
    "        self.cont_emb_notu_l2 = torch.nn.Linear(cont_emb_notu[0], cont_emb_notu[1])\n",
    "        self.cont_emb_notu = cont_emb_notu[1]\n",
    "            \n",
    "        ps = ifnone(ps, [0]*len(layers))\n",
    "        ps = listify(ps, layers)\n",
    "        \n",
    "        # Embedding for \"has one\" categorical features, cf #1 solution post\n",
    "        self.embeds = embedding(emb_szs[0][0], emb_szs[0][1])\n",
    "        \n",
    "        # At first we included information about the variable being processed (to extract feature importance). \n",
    "        # It works better using a constant feat (kind of intercept)\n",
    "        self.embeds_feat = embedding(201, 2)\n",
    "        self.embeds_feat_w = embedding(201, 2)\n",
    "        \n",
    "        self.emb_drop = nn.Dropout(emb_drop)\n",
    "        \n",
    "        n_emb = self.embeds.embedding_dim\n",
    "        n_emb_feat = self.embeds_feat.embedding_dim\n",
    "        n_emb_feat_w = self.embeds_feat_w.embedding_dim\n",
    "        \n",
    "        self.n_emb, self.n_emb_feat, self.n_emb_feat_w, self.n_cont,self.y_range = n_emb, n_emb_feat, n_emb_feat_w, n_cont, y_range\n",
    "        \n",
    "        sizes = self.get_sizes(layers, out_sz)\n",
    "        actns = [nn.ReLU(inplace=True)] * (len(sizes)-2) + [None]\n",
    "        layers = []\n",
    "        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n",
    "            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "            \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.seq = nn.Sequential()\n",
    "        \n",
    "        # Input size for the NN that predicts weights\n",
    "        inp_w = self.n_emb + self.n_emb_feat_w + self.cont_emb + self.cont_emb_notu\n",
    "        # Input size for the final NN that predicts output\n",
    "        inp_x = self.n_emb + self.cont_emb + self.cont_emb_notu\n",
    "        \n",
    "        # NN that predicts the weights\n",
    "        self.weight = nn.Linear(inp_w, 5)\n",
    "        self.weight2 = nn.Linear(5,1)\n",
    "        \n",
    "        mom = 0.1\n",
    "        self.bn_cat = nn.BatchNorm1d(200, momentum=mom)\n",
    "        self.bn_feat_emb = nn.BatchNorm1d(200, momentum=mom)\n",
    "        self.bn_feat_w = nn.BatchNorm1d(200, momentum=mom)\n",
    "        self.bn_raw = nn.BatchNorm1d(200, momentum=mom)\n",
    "        self.bn_notu = nn.BatchNorm1d(200, momentum=mom)\n",
    "        self.bn_w = nn.BatchNorm1d(inp_w, momentum=mom)\n",
    "        self.bn = nn.BatchNorm1d(inp_x, momentum=mom)\n",
    "        \n",
    "    def get_sizes(self, layers, out_sz):\n",
    "        return [self.n_emb + self.cont_emb_notu + self.cont_emb] + layers + [out_sz]\n",
    "\n",
    "    def forward(self, x_cat:Tensor, x_cont:Tensor) -> Tensor:\n",
    "        b_size = x_cont.size(0)\n",
    "        \n",
    "        # embedding of has one feat\n",
    "        x = [self.embeds(x_cat[:,i]) for i in range(200)]\n",
    "        x = torch.stack(x, dim=1)\n",
    "        \n",
    "        # embedding of intercept. It was embedding of feature id before\n",
    "        x_feat_emb = self.embeds_feat(x_cat[:,200])\n",
    "        x_feat_emb = torch.stack([x_feat_emb]*200, 1)\n",
    "        x_feat_emb = self.bn_feat_emb(x_feat_emb)\n",
    "        x_feat_w = self.embeds_feat_w(x_cat[:,200])\n",
    "        x_feat_w = torch.stack([x_feat_w]*200, 1)\n",
    "        \n",
    "        # \"continuous embedding\" of raw features\n",
    "        x_cont_raw = x_cont[:,:200].contiguous().view(-1, 1)\n",
    "        x_cont_raw = torch.cat([x_cont_raw, x_feat_emb.view(-1, self.n_emb_feat)], 1)\n",
    "        x_cont_raw = F.relu(self.cont_emb_l(x_cont_raw))\n",
    "        x_cont_raw = self.cont_emb_l2(x_cont_raw)\n",
    "        x_cont_raw = x_cont_raw.view(b_size, 200, self.cont_emb)\n",
    "        \n",
    "        # \"continuous embedding\" of not unique features\n",
    "        x_cont_notu = x_cont[:,200:].contiguous().view(-1, 1)\n",
    "        x_cont_notu = torch.cat([x_cont_notu, x_feat_emb.view(-1,self.n_emb_feat)], 1)\n",
    "        x_cont_notu = F.relu(self.cont_emb_notu_l(x_cont_notu))\n",
    "        x_cont_notu = self.cont_emb_notu_l2(x_cont_notu)\n",
    "        x_cont_notu = x_cont_notu.view(b_size, 200, self.cont_emb_notu)\n",
    "\n",
    "        x_cont_notu = self.bn_notu(x_cont_notu)\n",
    "        x = self.bn_cat(x)\n",
    "        x_cont_raw = self.bn_raw(x_cont_raw)\n",
    "\n",
    "        x = self.emb_drop(x)\n",
    "        x_cont_raw = self.emb_drop(x_cont_raw)\n",
    "        x_cont_notu = self.emb_drop(x_cont_notu)\n",
    "        x_feat_w = self.bn_feat_w(x_feat_w)\n",
    "        \n",
    "        # Predict a weight for each of the previous embeddings\n",
    "        x_w = torch.cat([x.view(-1,self.n_emb),\n",
    "                         x_feat_w.view(-1,self.n_emb_feat_w),\n",
    "                         x_cont_raw.view(-1, self.cont_emb), \n",
    "                         x_cont_notu.view(-1, self.cont_emb_notu)], 1)\n",
    "\n",
    "        x_w = self.bn_w(x_w)\n",
    "\n",
    "        w = F.relu(self.weight(x_w))\n",
    "        w = self.weight2(w).view(b_size, -1)\n",
    "        w = torch.nn.functional.softmax(w, dim=-1).unsqueeze(-1)\n",
    "\n",
    "        # weighted average of the differents embeddings using weights given by NN\n",
    "        x = (w * x).sum(dim=1)\n",
    "        x_cont_raw = (w * x_cont_raw).sum(dim=1)\n",
    "        x_cont_notu = (w * x_cont_notu).sum(dim=1)\n",
    "        \n",
    "        # Use NN on the weighted average to predict final output\n",
    "        x = torch.cat([x, x_cont_raw, x_cont_notu], 1) if self.n_emb != 0 else x_cont\n",
    "        x = self.bn(x)\n",
    "            \n",
    "        x = self.seq(x)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T22:04:20.005953Z",
     "start_time": "2019-04-26T22:04:19.998460Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    # python RNG\n",
    "    random.seed(seed)\n",
    "\n",
    "    # pytorch RNGs\n",
    "    import torch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # numpy RNG\n",
    "    import numpy as np\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T22:04:20.657764Z",
     "start_time": "2019-04-26T22:04:20.653924Z"
    }
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T22:04:21.094856Z",
     "start_time": "2019-04-26T22:04:21.086569Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2019-04-26 22:04:21,089:main:Input data\n"
     ]
    }
   ],
   "source": [
    "logger.info('Input data')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data = pd.read_feather('../input/create-data/921_data.fth')\n",
    "data = data.set_index('ID_code')\n",
    "\n",
    "etd = pd.read_feather('../input/create-data/921_etd.fth')\n",
    "etd = etd.set_index('ID_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T22:04:23.154874Z",
     "start_time": "2019-04-26T22:04:22.877821Z"
    }
   },
   "outputs": [],
   "source": [
    "data=pickle.load(open(\"/home/jd/data/santander/train-clean.p\", \"rb\" ) )\n",
    "#data = pd.read_feather('../input/create-data/921_data.fth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T22:04:24.073333Z",
     "start_time": "2019-04-26T22:04:23.990905Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data.set_index('ID_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T22:04:27.325953Z",
     "start_time": "2019-04-26T22:04:27.025360Z"
    }
   },
   "outputs": [],
   "source": [
    "#etd = pd.read_feather('../input/create-data/921_etd.fth')\n",
    "etd= pickle.load(open(\"/home/jd/data/santander/test-clean.p\", \"rb\" ) )\n",
    "etd = etd.set_index('ID_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T22:04:27.453290Z",
     "start_time": "2019-04-26T22:04:27.448021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218095"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T01:10:57.155664Z",
     "start_time": "2019-04-26T01:10:57.149656Z"
    }
   },
   "outputs": [],
   "source": [
    "has_one = [f'var_{i}_has_one' for i in range(200)]\n",
    "orig = [f'var_{i}' for i in range(200)]\n",
    "not_u = [f'var_{i}_not_unique' for i in range(200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T01:11:01.147843Z",
     "start_time": "2019-04-26T01:11:01.141634Z"
    }
   },
   "outputs": [],
   "source": [
    "cont_vars = orig + not_u\n",
    "cat_vars = has_one\n",
    "target = 'target'\n",
    "path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T01:11:02.566072Z",
     "start_time": "2019-04-26T01:11:02.558857Z"
    }
   },
   "outputs": [],
   "source": [
    "logger.info('cat treatment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T01:23:28.102318Z",
     "start_time": "2019-04-26T01:23:28.093202Z"
    }
   },
   "outputs": [],
   "source": [
    "data['var_0_has_one']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T01:24:21.774908Z",
     "start_time": "2019-04-26T01:24:21.732730Z"
    }
   },
   "outputs": [],
   "source": [
    "for f in cat_vars:\n",
    "    print (f)\n",
    "    data[f] = data[f].astype('category').cat.as_ordered()\n",
    "    etd[f] = pd.Categorical(etd[f], \n",
    "                            categories=data[f].cat.categories, \n",
    "                            ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant feature to replace feature index information\n",
    "feat = ['intercept']\n",
    "data['intercept'] = 1\n",
    "data['intercept'] = data['intercept'].astype('category')\n",
    "etd['intercept'] = 1\n",
    "etd['intercept'] = etd['intercept'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars += feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = pd.concat([data[cont_vars + cat_vars + ['target']], etd[cont_vars + cat_vars]])\n",
    "ref[cont_vars] = ss.fit_transform(ref[cont_vars].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ref.iloc[:200000]\n",
    "etd = ref.iloc[200000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[target] = data[target].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ref; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_seed = 42\n",
    "ss = StratifiedKFold(n_splits=10, random_state=fold_seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = []\n",
    "for num, (train,test) in enumerate(ss.split(data[target], data[target])):\n",
    "    folds.append([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[32]\n",
    "ps=0.2\n",
    "emb_drop=0.08\n",
    "cont_emb=(50,10)\n",
    "cont_emb_notu=(50,10)\n",
    "emb_szs = [[6,12]]\n",
    "use_bn = True\n",
    "joined=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code modified to sub with one seed\n",
    "seeds = [42] #, 1337, 666]\n",
    "\n",
    "results = []\n",
    "sub_preds = pd.DataFrame(columns=range(10), index=etd.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_fold, (train, test) in enumerate(folds):\n",
    "    procs=[]\n",
    "    df = (TabularList.from_df(data, path=path, cat_names=cat_vars, cont_names=cont_vars, procs=procs)\n",
    "                .split_by_idx(test)\n",
    "                .label_from_df(cols=target)\n",
    "            .add_test(TabularList.from_df(etd, path=path, cat_names=cat_vars, cont_names=cont_vars, procs=procs))\n",
    "            .databunch(num_workers=0, bs=1024))\n",
    "            \n",
    "    df.dls[0].dl = df.dls[0].new(sampler=LongerRandomSampler(data_source=df.train_ds, mult=2), shuffle=False).dl\n",
    "    for num_seed, seed in enumerate(seeds):\n",
    "        logger.info(f'Model {num_fold} seed {num_seed}')\n",
    "        set_seed(seed)\n",
    "        model = my_TabularModel(emb_szs, len(df.cont_names), out_sz=df.c, layers=layers, ps=ps, emb_drop=emb_drop,\n",
    "                                 y_range=None, use_bn=use_bn, cont_emb=cont_emb, cont_emb_notu=cont_emb_notu)\n",
    "\n",
    "        learn = Learner(df, model, metrics=None, callback_fns=AUROC, wd=0.1)\n",
    "        learn.fit_one_cycle(15, max_lr=1e-2, callbacks=[SaveModelCallback(learn, every='improvement', monitor='AUROC', name=f'fold{fold_seed}_{num_fold}_seed_{seed}'), AugShuffCallback(learn)])\n",
    "        pred, _ = learn.get_preds()\n",
    "        pred = pred[:,1]\n",
    "        \n",
    "        pred_test, _ = learn.get_preds(DatasetType.Test)\n",
    "        pred_test = pred_test[:,1]\n",
    "        \n",
    "        sub_preds.loc[:, num_fold] = pred_test\n",
    "        results.append(np.max(learn.recorder.metrics))\n",
    "        logger.info('result ' + str(results[-1]))\n",
    "        \n",
    "        np.save(f'oof_fold{fold_seed}_{num_fold}_seed_{seed}.npy', pred)\n",
    "        np.save(f'test_fold{fold_seed}_{num_fold}_seed_{seed}.npy', pred_test)\n",
    "        \n",
    "        del learn, pred, model, pred_test; gc.collect()\n",
    "    del df; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)\n",
    "print(np.mean(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_preds[target] = sub_preds.rank().mean(axis=1)\n",
    "sub_preds[[target]].to_csv('submission_NN_wo_pseudo_seed42.csv', index_label='ID_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "notify_time": "10"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
